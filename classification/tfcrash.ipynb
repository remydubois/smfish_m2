{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json \n",
    "import numpy\n",
    "\n",
    "simulations_path = '/Users/remydubois/Dropbox/Remy/Data/Simulations-1/smFISH_simulations__batch_0001.json'\n",
    "cell_path = '/Users/remydubois/Dropbox/Remy/Data/cellLibrary.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "py_func() missing 2 required positional arguments: 'inp' and 'Tout'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8cc16a7a34e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testjson.json'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ds = ds.map(tf.as_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Here four threads for preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# ds = ds.batch()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: py_func() missing 2 required positional arguments: 'inp' and 'Tout'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with open(cell_path) as f:\n",
    "    jsoncells = json.load(f)\n",
    "with open('testjson.json', 'w+') as f:\n",
    "    f.write(\"{'a':1}\\n{'b':2}\")\n",
    "    # text = f.read()\n",
    "# f = open(simulations_path)\n",
    "# g = open(cell_path)\n",
    "\n",
    "# ds = tf.data.Dataset.from_tensor_slices(g.read())\n",
    "ds = tf.data.TextLineDataset(['testjson.json'])\n",
    "# ds = ds.map(tf.as_string)\n",
    "ds = ds.map(tf.py_func(lambda x: json.loads(x), ))  # Here four threads for preprocessing\n",
    "# ds = ds.batch()\n",
    "# or\n",
    "# ds = ds.apply(tf.contrib.data.map_and_batch(\n",
    "#     map_func=parse_fn, batch_size=FLAGS.batch_size, num_parallel_batches=4))  # Not sure perfectly equivalent\n",
    "# ds = ds.prefetch(1)  # Allow consumer and producer to overlap their work, precise here the queue size.\n",
    "# ds = ds.repeat(None)\n",
    "# or \n",
    "# ds = tf.contrib.data.shuffle_and_repeat(ds)\n",
    "it = ds.make_one_shot_iterator()\n",
    "ex = it.get_next()\n",
    "sess = tf.Session()\n",
    "print(sess.run(ex))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'decode'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-449a8df4f998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{'a':1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_json_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'decode'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x = tf.constant(\"{'a':1}\")\n",
    "y = tf.decode_json_example()\n",
    "sess = tf.Session()\n",
    "print(sess.run(x).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter('/Users/remydubois/Desktop/testTFRecord.tfrecords')\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "pos = [[0, 1], [1, 2]]\n",
    "feature = {'positions': _bytes_feature(tf.compat.as_bytes(str(pos).encode())),\n",
    "           'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=[2]))\n",
    "           }\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DecodeRaw_2:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/remydubois/Desktop/testTFRecord.tfrecords'  # address to save the hdf5 file\n",
    "with tf.Session() as sess:\n",
    "    feature = {'positions': tf.FixedLenFeature([], tf.string),\n",
    "               'shape': tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['positions'], tf.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1]], dtype=int32), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 0],\n       [1, 1]], dtype=int32), 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "data = [[[1, 1], [2, 2]], [[0, 0], [1, 0], [1, 1]]]\n",
    "labels = [0, 1]\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "with tf.python_io.TFRecordWriter('testTFRecord.tfrecord') as writer:\n",
    "    for d, l in zip(data, labels):\n",
    "        image_raw = numpy.array(d, dtype=numpy.int32).ravel().tostring()\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    'image_raw': _bytes_feature(image_raw),\n",
    "                    'label': _int64_feature(l)\n",
    "                }))\n",
    "        writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "def decode(serialized_example):\n",
    "    \"\"\"Parses an image and label from the given `serialized_example`.\"\"\"\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "    # [mnist.IMAGE_PIXELS].\n",
    "    image = tf.decode_raw(features['image_raw'], tf.int32)\n",
    "    out = tf.reshape(image, (-1, 2))\n",
    "    \n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    return out, label\n",
    "\n",
    "\n",
    "ds = tf.data.TFRecordDataset('testTFRecord.tfrecord')\n",
    "ds = ds.map(decode)\n",
    "ds = ds.map(lambda x, l: (tf.sparse_to_dense(x, (tf.reduce_max(x) + 1, ) * 2, 1, validate_indices=False), l))\n",
    "# ds = ds.map(lambda x: tf.pad(x, tf.substr([[5,5], [5,5]], tf.shape(x))))\n",
    "it = ds.make_one_shot_iterator()\n",
    "print(sess.run(it.get_next()))\n",
    "print(sess.run(it.get_next()))\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(numpy.load('testmat.npy'))\n",
    "# dataset = dataset.map(lambda x: tf.sparse_to_dense(x, (2, 2), 1))\n",
    "# it = dataset.make_one_shot_iterator()\n",
    "# print(sess.run(it.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "class TFRecordBuilder(luigi.task):\n",
    "    def requires(self):\n",
    "        return [Merge()]\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget('PositionsTFRecord.tfrecord')\n",
    "\n",
    "    def run(self):\n",
    "        with tf.python_io.TFRecordWriter('testTFRecord.tfrecord') as writer, open(self.input()[0]) as fin:\n",
    "            # Or read by chunk with pandas to not fit it all in memory\n",
    "            js = json.load(fin)\n",
    "            for item in js:\n",
    "                d = item['positions']\n",
    "                l = item['label']\n",
    "                image_raw = numpy.array(d, dtype=numpy.int32).ravel().tostring()\n",
    "                example = tf.train.Example(\n",
    "                    features=tf.train.Features(\n",
    "                        feature={\n",
    "                            'image_raw': _bytes_feature(image_raw),\n",
    "                            'label': _int64_feature(l)\n",
    "                        }))\n",
    "                writer.write(example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import luigi\n",
    "import mmap\n",
    "import pandas\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class StackSimulations(luigi.Task):\n",
    "    def requires(self):\n",
    "        return []\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"stacked_file.json\")\n",
    "\n",
    "    def run(self):\n",
    "        simulation_batches_path = [simulations_path + f for f in os.listdir(simulations_path) if\n",
    "                                   f.lower().endswith('.json')]\n",
    "\n",
    "        # Combine all the simulations\n",
    "        simulations = []\n",
    "        for b in simulation_batches_path:\n",
    "            with open(b, 'r') as f:\n",
    "                simulation_batch = json.load(f)\n",
    "            simulations.extend(simulation_batch)\n",
    "        dataset = pandas.DataFrame(simulations)\n",
    "            \n",
    "        dataset = dataset[['cell_ID', 'RNA_pos', 'pattern_name']]\n",
    "\n",
    "        # Now shuffle\n",
    "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        dataset.to_json(simulations_path + 'stacked_file.json', orient='records', lines=True)\n",
    "\n",
    "\n",
    "class Merge(luigi.task):\n",
    "    def requires(self):\n",
    "        return [StackSimulations()]\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"merged_file.json\")\n",
    "    \n",
    "    def run(self):\n",
    "        simulations = pandas.read_json(self.input()[0], orient='records', lines=True)\n",
    "        jsoncells = pandas.read_json(cell_path)\n",
    "        jsoncells['cell_ID'] = jsoncells.index\n",
    "        merged = simulations.join(jsoncells, 'cell_ID')\n",
    "        merged['pos'] = merged.pos_nuc + merged.pos_cell + merged.RNA_pos\n",
    "        le = LabelEncoder()\n",
    "        merged['pattern_name'] = le.fit_transform(merged['pattern_name'])\n",
    "        merged = merged[['pos', 'pattern_name']]\n",
    "        with self.output().open('w') as fout:\n",
    "            merged.to_json(fout, orient='index')\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
